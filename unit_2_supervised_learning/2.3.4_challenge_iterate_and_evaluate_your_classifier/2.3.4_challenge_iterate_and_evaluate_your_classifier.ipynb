{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "It's time to revisit your classifier from the previous assignment. Using the evaluation techniques we've covered here, look at your classifier's performance in more detail. Then go back and iterate by engineering new features, removing poor features, or tuning parameters. Repeat this process until you have five different versions of your classifier. Once you've iterated, answer these questions to compare the performance of each:\n",
    "<br>Do any of your classifiers seem to overfit?\n",
    "<br>Which seem to perform the best? Why?\n",
    "<br>Which features seemed to be most impactful to performance?\n",
    "### Outline\n",
    "Version 1: Original\n",
    "<br>Version 2: Words in both sentiments\n",
    "<br>Version 3: Only positive adjectives\n",
    "<br>Version 4: Only positive verbs\n",
    "<br>Version 5: Only positive adverbs\n",
    "<br>Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "data_path = \"data/imdb_labelled.txt\"\n",
    "reviews_raw = pd.read_csv(data_path, delimiter= '\\t', header=None)\n",
    "reviews_raw.columns = ['review', 'positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_features(df, keywords):\n",
    "    df_keywords = df.copy()\n",
    "    for key in keywords:\n",
    "    # Note that we add spaces around the key so that we're getting the word,\n",
    "    # not just pattern matching.\n",
    "        df_keywords[str(key)] = df_keywords.review.str.contains(\n",
    "            ' ' + str(key) + ' ',\n",
    "            case=False\n",
    "        )\n",
    "    return df_keywords\n",
    "\n",
    "def evaluate_version(df, keywords):\n",
    "    random.seed(120)\n",
    "    bnb = BernoulliNB()\n",
    "    data = df[keywords]\n",
    "    target = df['positive']\n",
    "    total_points = data.shape[0]\n",
    "    # baseline\n",
    "    baseline = (target == True).sum()\n",
    "    train_test_score = bnb.fit(data, target).score(data, target)\n",
    "    print (\"Baseline positive: {}\".format(round(1 - baseline / total_points, 3) ))\n",
    "    print (\"Train = Test Score: {}\".format(round(train_test_score, 3)))\n",
    "    # folds\n",
    "    random_array = list(range(total_points))\n",
    "    random.shuffle(random_array)\n",
    "    random_series = pd.Series(random_array)\n",
    "    folds = 10\n",
    "    fold_scores = np.zeros(folds)\n",
    "    fold_sens = np.zeros(folds)\n",
    "    fold_spec = np.zeros(folds)\n",
    "    fold_size = int(total_points / folds)\n",
    "    \n",
    "    # cross validation\n",
    "    for fold in range(folds):\n",
    "        test_vals = list(random_series[fold * fold_size:(fold + 1) * fold_size])\n",
    "        train_vals = list(random_series)\n",
    "        train_vals = list(set(train_vals) - set(test_vals))\n",
    "        fold_X_train = data.iloc[train_vals]\n",
    "        fold_X_test = data.iloc[test_vals]\n",
    "        fold_Y_train = target.iloc[train_vals]\n",
    "        fold_Y_test = target.iloc[test_vals]\n",
    "        fold_fit = bnb.fit(fold_X_train, fold_Y_train)\n",
    "        fold_pred = bnb.predict(fold_X_test)\n",
    "        fold_scores[fold] = fold_fit.score(fold_X_test, fold_Y_test)\n",
    "        # confusion matrix\n",
    "        spam_confusion_matrix = confusion_matrix(fold_Y_test, fold_pred)\n",
    "        # sensitivity / specificity\n",
    "        fold_sens[fold] = round(spam_confusion_matrix[0][0] / (spam_confusion_matrix[0][0] + spam_confusion_matrix[0][1]), 3)\n",
    "        fold_spec[fold] = round(spam_confusion_matrix[1][0] / (spam_confusion_matrix[1][0] + spam_confusion_matrix[1][1]), 3)\n",
    "    fold_df = pd.DataFrame(data = np.transpose([fold_scores, fold_sens, fold_spec]), columns = ['Score', 'Sensitivity', 'Specificity'])\n",
    "    print (fold_df)\n",
    "    print (\"Means\\n{}\".format(fold_df.mean()))\n",
    "    print (\"Standard Deviations\\n{}\".format(fold_df.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1: Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline positive: 0.484\n",
      "Train = Test Score: 0.618\n",
      "      Score  Sensitivity  Specificity\n",
      "0  0.581081        0.933        0.659\n",
      "1  0.594595        0.971        0.725\n",
      "2  0.540541        1.000        0.791\n",
      "3  0.689189        0.907        0.613\n",
      "4  0.635135        0.919        0.649\n",
      "5  0.662162        1.000        0.641\n",
      "6  0.581081        0.923        0.800\n",
      "7  0.662162        1.000        0.641\n",
      "8  0.608108        0.865        0.649\n",
      "9  0.540541        0.917        0.816\n",
      "Means\n",
      "Score          0.609459\n",
      "Sensitivity    0.943500\n",
      "Specificity    0.698400\n",
      "dtype: float64\n",
      "Standard Deviations\n",
      "Score          0.051537\n",
      "Sensitivity    0.046739\n",
      "Specificity    0.077280\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "keywords_v1 = ['superb', 'amazing', 'interesting', 'good', 'great', 'best', 'love', 'perfect', 'masterpiece', 'beautiful', 'excellent', 'wonderful', 'art', 'like', 'liked', 'enjoy', 'enjoyed']\n",
    "\n",
    "reviews_v1 = keyword_features(reviews_raw, keywords_v1)\n",
    "\n",
    "evaluate_version(reviews_v1, keywords_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 2: Words in both sentiments\n",
    "Our first iteration tells us that these keywords adequately predict positive sentiments correctly. However, the specificity score is quite low, meaning these keywords do not predict negative sentiments correctly. This must mean that there are keywords in our list which are present in negative sentiment reviews. For version 2, let's take some words out that might be present in both negative and positive sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline positive: 0.484\n",
      "Train = Test Score: 0.588\n",
      "      Score  Sensitivity  Specificity\n",
      "0  0.527027        0.933        0.750\n",
      "1  0.581081        1.000        0.775\n",
      "2  0.527027        1.000        0.814\n",
      "3  0.675676        0.953        0.710\n",
      "4  0.608108        0.946        0.730\n",
      "5  0.594595        1.000        0.769\n",
      "6  0.581081        0.949        0.829\n",
      "7  0.581081        1.000        0.795\n",
      "8  0.594595        0.919        0.730\n",
      "9  0.527027        0.972        0.895\n",
      "Means\n",
      "Score          0.57973\n",
      "Sensitivity    0.96720\n",
      "Specificity    0.77970\n",
      "dtype: float64\n",
      "Standard Deviations\n",
      "Score          0.045694\n",
      "Sensitivity    0.031272\n",
      "Specificity    0.055730\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "keywords_v2 = ['superb', 'amazing', 'great', 'best', 'love', 'perfect', 'masterpiece', 'beautiful', 'excellent', 'wonderful', 'liked', 'enjoy', 'enjoyed']\n",
    "\n",
    "reviews_v2 = keyword_features(reviews_raw, keywords_v2)\n",
    "\n",
    "evaluate_version(reviews_v2, keywords_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 3: Only positive adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline positive: 0.484\n",
      "Train = Test Score: 0.56\n",
      "      Score  Sensitivity  Specificity\n",
      "0  0.459459        0.967        0.886\n",
      "1  0.527027        1.000        0.875\n",
      "2  0.527027        1.000        0.814\n",
      "3  0.675676        0.977        0.742\n",
      "4  0.594595        1.000        0.811\n",
      "5  0.554054        1.000        0.846\n",
      "6  0.554054        0.974        0.914\n",
      "7  0.554054        1.000        0.846\n",
      "8  0.567568        0.946        0.811\n",
      "9  0.500000        0.972        0.947\n",
      "Means\n",
      "Score          0.551351\n",
      "Sensitivity    0.983600\n",
      "Specificity    0.849200\n",
      "dtype: float64\n",
      "Standard Deviations\n",
      "Score          0.057615\n",
      "Sensitivity    0.019161\n",
      "Specificity    0.059117\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "keywords_v3 = ['superb', 'amazing', 'great', 'perfect', 'masterpiece', 'beautiful', 'excellent', 'wonderful', 'outstanding', 'exceptional', 'marvelous', 'magnificent', 'preeminent', 'first-rate', 'terrific', 'tremendous', 'fantastic', 'fabulous']\n",
    "\n",
    "reviews_v3 = keyword_features(reviews_raw, keywords_v3)\n",
    "\n",
    "evaluate_version(reviews_v3, keywords_v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 4: Only positive verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline positive: 0.484\n",
      "Train = Test Score: 0.524\n",
      "      Score  Sensitivity  Specificity\n",
      "0  0.445946        0.967        0.909\n",
      "1  0.472973        0.882        0.875\n",
      "2  0.472973        1.000        0.907\n",
      "3  0.418919        0.023        0.032\n",
      "4  0.513514        0.054        0.027\n",
      "5  0.500000        0.000        0.051\n",
      "6  0.459459        0.051        0.086\n",
      "7  0.513514        1.000        0.923\n",
      "8  0.527027        0.946        0.892\n",
      "9  0.472973        0.944        0.974\n",
      "Means\n",
      "Score          0.47973\n",
      "Sensitivity    0.58670\n",
      "Specificity    0.56760\n",
      "dtype: float64\n",
      "Standard Deviations\n",
      "Score          0.033859\n",
      "Sensitivity    0.478765\n",
      "Specificity    0.447322\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "keywords_v4 = ['like', 'liked', 'love', 'loved', 'enjoy', 'enjoyed', 'adore', 'adored', 'appreciate', 'appreciated', 'respect', 'respected', 'approve', 'approved']\n",
    "\n",
    "reviews_v4 = keyword_features(reviews_raw, keywords_v4)\n",
    "\n",
    "evaluate_version(reviews_v4, keywords_v4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 5: Only positive adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline positive: 0.484\n",
      "Train = Test Score: 0.516\n",
      "      Score  Sensitivity  Specificity\n",
      "0  0.594595        0.000        0.000\n",
      "1  0.527027        0.000        0.025\n",
      "2  0.540541        0.000        0.070\n",
      "3  0.405405        0.000        0.032\n",
      "4  0.500000        0.000        0.000\n",
      "5  0.527027        0.029        0.026\n",
      "6  0.472973        0.000        0.000\n",
      "7  0.527027        0.000        0.000\n",
      "8  0.500000        0.000        0.000\n",
      "9  0.500000        0.000        0.026\n",
      "Means\n",
      "Score          0.509459\n",
      "Sensitivity    0.002900\n",
      "Specificity    0.017900\n",
      "dtype: float64\n",
      "Standard Deviations\n",
      "Score          0.048952\n",
      "Sensitivity    0.009171\n",
      "Specificity    0.022845\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "keywords_v5 = ['superbly', 'amazingly', 'greatly', 'perfectly', 'beautifully', 'excellently', 'wonderfully', 'outstandingly', 'exceptionally', 'marvelously', 'magnificently', 'preeminently', 'terrifically', 'tremendously', 'fantastically', 'fabulously']\n",
    "\n",
    "reviews_v5 = keyword_features(reviews_raw, keywords_v5)\n",
    "\n",
    "evaluate_version(reviews_v5, keywords_v5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write-up\n",
    "The original version, which combines the different ways to praise a product, remains the most accurate, albeit a low specificity. The keywords used here are mostly positive and vary between adjectives, nouns and verbs. The low specificity must be due to some keywords being used in negative reviews.\n",
    "<br>Each version performs worse than the original, but has unique attributes and can be learned from. At the cost of a lower score, the version which removes ambiguous words from the original increases the sensitivity and specificity. \n",
    "<br>The adjective-only version achieves the highest average sensitivities and specificities. This is probably because positive reviews use positive adjectives and negative reviews don't. The verb-only version scores well, but has very low sensitivity and specificity. This is most likely due to the scarcity of these verbs in the reviews.\n",
    "<br>The adverb-only version performs horribly, but still beats the baseline positive score. This model is slightly better than guessing, illustrated by the almost-zero sensitivity and specificity.\n",
    "<br>If I were to use this model in the wild, I would choose Version 2 for its accuracy in Type 1 and Type 2 errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
