{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The task here is to compare neural network architectures to each other using their training time and accuracy. The dataset consists of individual drum samples as one might use in a DAW to build a beat. The baseline accuracy is given by a SVM classifier on the MFCC features. Since these features are often displayed as images, and can in fact be processed as images, I suspect a complex CNN will perform best. Another neural network architecture that may work is an LSTM RNN which is most often used for time series problems.\n",
    "\n",
    "### Outline\n",
    "- EDA\n",
    "- Modeling\n",
    "- Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "# supervised learning\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "#neural networks\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout,Flatten, Conv2D, MaxPooling2D,LSTM, Input, TimeDistributed\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2642 files in 67.45 seconds\n"
     ]
    }
   ],
   "source": [
    "sounds = {\"kick\":\"kick\",\n",
    "          \"kik\":\"kick\",\n",
    "          \"snare\":\"snare\",\n",
    "          \"snr\":\"snare\",\n",
    "          \"bongo\":\"bongo\",\n",
    "          \"tom\":\"tom\",\n",
    "          \"hat\":\"hihat\",\n",
    "          \"hh\":\"hihat\",\n",
    "          \"ride\":\"ride\",\n",
    "          \"clap\":\"clap\",\n",
    "          \"cow\":\"cowbell\",\n",
    "          \"bell\":\"cowbell\",\n",
    "          \"conga\":\"conga\"\n",
    "         }\n",
    "kits,sound_profiles,files,lengths,mfccs = [],[],[],[],[]\n",
    "data_path = \"data/drums/\"\n",
    "start_time=time.time()\n",
    "for kit_path in os.listdir(data_path):\n",
    "    kit = os.listdir(data_path + \"/\" + kit_path)\n",
    "    sound_list = [[x,[sounds[sound] for sound in sounds if sound in x.lower()]] for x in kit]\n",
    "    sound_list = np.array([[x[0],x[1][0]] for x in sound_list if x[1]])\n",
    "    #print(kit_path,\"------------------\",sound_list)\n",
    "    if len(sound_list)>0:\n",
    "        for sound_path,sound_profile in sound_list:\n",
    "            # add files, kits, sound_profiles\n",
    "            files.append(sound_path)\n",
    "            kits.append(kit_path)\n",
    "            sound_profiles.append(sound_profile)\n",
    "            # load data\n",
    "            data,Fs = librosa.load(data_path + kit_path + \"/\" + sound_path,sr=None)\n",
    "            # find longest file: 131811 S\n",
    "            lengths.append(len(data))\n",
    "            # zero pad or cut to 2 seconds\n",
    "            if len(data) > Fs*2: \n",
    "                data = data[:Fs*2]\n",
    "            elif len(data) < Fs*2: \n",
    "                data = np.append(data,[0]*(Fs*2-len(data)))\n",
    "            # process wav\n",
    "            mfcc_temp = librosa.feature.mfcc(np.array(data),sr=Fs,n_mfcc=40, n_fft=2048, hop_length=512)\n",
    "            mfcc_slice = mfcc_temp[1:13,:]\n",
    "            mfccs.append(mfcc_slice.ravel())\n",
    "print(\"Processed\",len(lengths),\"files in {:0.2f}\".format(time.time()-start_time),\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(mfccs)\n",
    "X = df.values\n",
    "df['file']=files\n",
    "df['kit']=kits\n",
    "df['sound_profile']=sound_profiles\n",
    "Y = pd.factorize(sound_profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# target distribution\n",
    "sound_count = df['sound_profile'].value_counts()\n",
    "plt.bar(range(len(sound_count)),sound_count,tick_label=sound_count.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Accuracy 0.6998 +/- 0.0241 in 84.97 seconds\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(C=100,kernel='linear')\n",
    "lg = LogisticRegression(C=1,penalty='l2')\n",
    "def evaluate(model):\n",
    "    score = cross_val_score(model,X,Y[0])\n",
    "    print(\"SVC Accuracy {:0.4f} +/- {:0.4f} in {:0.2f} seconds\".format(score.mean(),score.std(),time.time()-start_time))\n",
    "evaluate(svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare neural network variables\n",
    "num_classes = len(sound_count)\n",
    "Y_nn = to_categorical(Y[0], num_classes)\n",
    "# cnn\n",
    "img_rows, img_cols = mfcc_slice.shape[1], mfcc_slice.shape[0]\n",
    "X_cnn = X.reshape(X.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "# train test split\n",
    "X_train, X_test,X_cnn_train,X_cnn_test, Y_train, Y_test = train_test_split(X,X_cnn,Y_nn,test_size=.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build simple mlp\n",
    "mlp = Sequential()\n",
    "mlp.add(Dense(128,activation='relu',input_dim=X.shape[1]))\n",
    "mlp.add(Dropout(.33))\n",
    "mlp.add(Dense(num_classes,activation='softmax'))\n",
    "mlp.compile(loss=\"categorical_crossentropy\",\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1185 samples, validate on 585 samples\n",
      "Epoch 1/10\n",
      "1185/1185 [==============================] - 0s 258us/step - loss: 9.2830 - acc: 0.3586 - val_loss: 6.3186 - val_acc: 0.5624\n",
      "Epoch 2/10\n",
      "1185/1185 [==============================] - 0s 98us/step - loss: 6.7699 - acc: 0.5342 - val_loss: 5.1804 - val_acc: 0.6188\n",
      "Epoch 3/10\n",
      "1185/1185 [==============================] - 0s 101us/step - loss: 5.1805 - acc: 0.6135 - val_loss: 3.8132 - val_acc: 0.6855\n",
      "Epoch 4/10\n",
      "1185/1185 [==============================] - 0s 101us/step - loss: 4.2356 - acc: 0.6827 - val_loss: 3.8532 - val_acc: 0.7111\n",
      "Epoch 5/10\n",
      "1185/1185 [==============================] - 0s 103us/step - loss: 4.0829 - acc: 0.7148 - val_loss: 3.5378 - val_acc: 0.7316\n",
      "Epoch 6/10\n",
      "1185/1185 [==============================] - 0s 97us/step - loss: 3.7148 - acc: 0.7257 - val_loss: 3.3968 - val_acc: 0.7436\n",
      "Epoch 7/10\n",
      "1185/1185 [==============================] - 0s 106us/step - loss: 3.4682 - acc: 0.7468 - val_loss: 3.1443 - val_acc: 0.7624\n",
      "Epoch 8/10\n",
      "1185/1185 [==============================] - 0s 138us/step - loss: 3.7444 - acc: 0.7308 - val_loss: 3.2918 - val_acc: 0.7385\n",
      "Epoch 9/10\n",
      "1185/1185 [==============================] - 0s 138us/step - loss: 3.3522 - acc: 0.7435 - val_loss: 3.0630 - val_acc: 0.7812\n",
      "Epoch 10/10\n",
      "1185/1185 [==============================] - 0s 136us/step - loss: 3.5451 - acc: 0.7333 - val_loss: 3.0555 - val_acc: 0.7692\n"
     ]
    }
   ],
   "source": [
    "# evluate simple mlp\n",
    "mlp_history = mlp.fit(X_train,Y_train,\n",
    "                      batch_size=128,\n",
    "                      epochs=10,\n",
    "                      verbose=1,\n",
    "                      validation_split=.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Complex MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build complex mlp\n",
    "cmlp = Sequential()\n",
    "cmlp.add(Dense(128,activation='relu',input_dim=X.shape[1]))\n",
    "cmlp.add(Dropout(.5))\n",
    "cmlp.add(Dense(128,activation='relu'))\n",
    "cmlp.add(Dropout(.25))\n",
    "cmlp.add(Dense(64,activation='relu'))\n",
    "cmlp.add(Dropout(.25))\n",
    "cmlp.add(Dense(num_classes,activation='softmax'))\n",
    "cmlp.compile(loss=\"categorical_crossentropy\",\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1185 samples, validate on 585 samples\n",
      "Epoch 1/100\n",
      "1185/1185 [==============================] - 0s 120us/step - loss: 1.2065 - acc: 0.7072 - val_loss: 0.9099 - val_acc: 0.7949\n",
      "Epoch 2/100\n",
      "1185/1185 [==============================] - 0s 116us/step - loss: 1.0811 - acc: 0.7156 - val_loss: 0.9288 - val_acc: 0.7795\n",
      "Epoch 3/100\n",
      "1185/1185 [==============================] - 0s 117us/step - loss: 1.1054 - acc: 0.7063 - val_loss: 0.9231 - val_acc: 0.7880\n",
      "Epoch 4/100\n",
      "1185/1185 [==============================] - 0s 107us/step - loss: 1.1043 - acc: 0.7122 - val_loss: 0.9338 - val_acc: 0.7846\n",
      "Epoch 5/100\n",
      "1185/1185 [==============================] - 0s 108us/step - loss: 1.1769 - acc: 0.6954 - val_loss: 0.9241 - val_acc: 0.8068\n",
      "Epoch 6/100\n",
      "1185/1185 [==============================] - 0s 117us/step - loss: 1.0983 - acc: 0.7072 - val_loss: 0.8405 - val_acc: 0.8034\n",
      "Epoch 7/100\n",
      "1185/1185 [==============================] - 0s 116us/step - loss: 1.1229 - acc: 0.7114 - val_loss: 0.8485 - val_acc: 0.7897\n",
      "Epoch 8/100\n",
      "1185/1185 [==============================] - 0s 115us/step - loss: 1.1297 - acc: 0.7266 - val_loss: 0.8408 - val_acc: 0.7829\n",
      "Epoch 9/100\n",
      "1185/1185 [==============================] - 0s 110us/step - loss: 1.0634 - acc: 0.7291 - val_loss: 0.8430 - val_acc: 0.7812\n",
      "Epoch 10/100\n",
      "1185/1185 [==============================] - 0s 122us/step - loss: 0.9848 - acc: 0.7300 - val_loss: 0.8537 - val_acc: 0.7932\n",
      "Epoch 11/100\n",
      "1185/1185 [==============================] - 0s 113us/step - loss: 1.0528 - acc: 0.7316 - val_loss: 0.9034 - val_acc: 0.7846\n",
      "Epoch 12/100\n",
      "1185/1185 [==============================] - 0s 112us/step - loss: 1.0083 - acc: 0.7333 - val_loss: 0.8534 - val_acc: 0.7812\n",
      "Epoch 13/100\n",
      "1185/1185 [==============================] - 0s 114us/step - loss: 1.1046 - acc: 0.7215 - val_loss: 0.8606 - val_acc: 0.7846\n",
      "Epoch 14/100\n",
      "1185/1185 [==============================] - 0s 111us/step - loss: 1.0477 - acc: 0.7392 - val_loss: 0.8339 - val_acc: 0.7880\n",
      "Epoch 15/100\n",
      "1185/1185 [==============================] - 0s 124us/step - loss: 1.0542 - acc: 0.7536 - val_loss: 0.8285 - val_acc: 0.7846\n",
      "Epoch 16/100\n",
      "1185/1185 [==============================] - 0s 108us/step - loss: 0.9665 - acc: 0.7502 - val_loss: 0.8185 - val_acc: 0.7897\n",
      "Epoch 17/100\n",
      "1185/1185 [==============================] - 0s 124us/step - loss: 0.8962 - acc: 0.7392 - val_loss: 0.7906 - val_acc: 0.8017\n",
      "Epoch 18/100\n",
      "1185/1185 [==============================] - 0s 111us/step - loss: 0.9415 - acc: 0.7629 - val_loss: 0.7805 - val_acc: 0.7863\n",
      "Epoch 19/100\n",
      "1185/1185 [==============================] - 0s 112us/step - loss: 0.9482 - acc: 0.7511 - val_loss: 0.7833 - val_acc: 0.7726\n",
      "Epoch 20/100\n",
      "1185/1185 [==============================] - 0s 114us/step - loss: 0.9245 - acc: 0.7629 - val_loss: 0.8025 - val_acc: 0.7846\n",
      "Epoch 21/100\n",
      "1185/1185 [==============================] - 0s 114us/step - loss: 0.9264 - acc: 0.7603 - val_loss: 0.8139 - val_acc: 0.7897\n",
      "Epoch 22/100\n",
      "1185/1185 [==============================] - 0s 119us/step - loss: 0.8928 - acc: 0.7620 - val_loss: 0.8219 - val_acc: 0.7915\n",
      "Epoch 23/100\n",
      "1185/1185 [==============================] - 0s 113us/step - loss: 0.9202 - acc: 0.7367 - val_loss: 0.8148 - val_acc: 0.7966\n",
      "Epoch 24/100\n",
      "1185/1185 [==============================] - 0s 113us/step - loss: 0.8957 - acc: 0.7603 - val_loss: 0.8040 - val_acc: 0.7880\n",
      "Epoch 25/100\n",
      "1185/1185 [==============================] - 0s 115us/step - loss: 0.8183 - acc: 0.7738 - val_loss: 0.7823 - val_acc: 0.7915\n",
      "Epoch 26/100\n",
      "1185/1185 [==============================] - 0s 124us/step - loss: 0.8658 - acc: 0.7578 - val_loss: 0.7613 - val_acc: 0.7983\n",
      "Epoch 27/100\n",
      "1185/1185 [==============================] - 0s 112us/step - loss: 0.8560 - acc: 0.7595 - val_loss: 0.7614 - val_acc: 0.7880\n",
      "Epoch 28/100\n",
      "1185/1185 [==============================] - 0s 113us/step - loss: 0.8619 - acc: 0.7747 - val_loss: 0.7908 - val_acc: 0.7932\n",
      "Epoch 29/100\n",
      "1185/1185 [==============================] - 0s 112us/step - loss: 0.8142 - acc: 0.7654 - val_loss: 0.7816 - val_acc: 0.7966\n",
      "Epoch 30/100\n",
      "1185/1185 [==============================] - 0s 118us/step - loss: 0.7678 - acc: 0.7907 - val_loss: 0.7689 - val_acc: 0.7983\n",
      "Epoch 31/100\n",
      "1185/1185 [==============================] - 0s 111us/step - loss: 0.8948 - acc: 0.7468 - val_loss: 0.7712 - val_acc: 0.7846\n",
      "Epoch 32/100\n",
      "1185/1185 [==============================] - 0s 121us/step - loss: 0.7905 - acc: 0.7696 - val_loss: 0.8014 - val_acc: 0.7897\n",
      "Epoch 33/100\n",
      "1185/1185 [==============================] - 0s 113us/step - loss: 0.8039 - acc: 0.7713 - val_loss: 0.8090 - val_acc: 0.7983\n",
      "Epoch 34/100\n",
      "1185/1185 [==============================] - 0s 123us/step - loss: 0.7896 - acc: 0.7561 - val_loss: 0.7910 - val_acc: 0.7949\n",
      "Epoch 35/100\n",
      "1185/1185 [==============================] - 0s 122us/step - loss: 0.7966 - acc: 0.7823 - val_loss: 0.7971 - val_acc: 0.7932\n",
      "Epoch 36/100\n",
      "1185/1185 [==============================] - 0s 116us/step - loss: 0.7376 - acc: 0.7713 - val_loss: 0.7918 - val_acc: 0.7863\n",
      "Epoch 37/100\n",
      "1185/1185 [==============================] - 0s 112us/step - loss: 0.7804 - acc: 0.7755 - val_loss: 0.8090 - val_acc: 0.7897\n",
      "Epoch 38/100\n",
      "1185/1185 [==============================] - 0s 129us/step - loss: 0.7748 - acc: 0.7907 - val_loss: 0.7998 - val_acc: 0.8034\n",
      "Epoch 39/100\n",
      "1185/1185 [==============================] - 0s 165us/step - loss: 0.7519 - acc: 0.7747 - val_loss: 0.7833 - val_acc: 0.8034\n",
      "Epoch 40/100\n",
      "1185/1185 [==============================] - 0s 168us/step - loss: 0.7797 - acc: 0.7747 - val_loss: 0.7905 - val_acc: 0.7983\n",
      "Epoch 41/100\n",
      "1185/1185 [==============================] - 0s 163us/step - loss: 0.7223 - acc: 0.7764 - val_loss: 0.7837 - val_acc: 0.8034\n",
      "Epoch 42/100\n",
      "1185/1185 [==============================] - 0s 133us/step - loss: 0.6592 - acc: 0.7975 - val_loss: 0.7863 - val_acc: 0.8068\n",
      "Epoch 43/100\n",
      "1185/1185 [==============================] - 0s 108us/step - loss: 0.7576 - acc: 0.7747 - val_loss: 0.7653 - val_acc: 0.8051\n",
      "Epoch 44/100\n",
      "1185/1185 [==============================] - 0s 129us/step - loss: 0.7825 - acc: 0.7789 - val_loss: 0.7724 - val_acc: 0.8103\n",
      "Epoch 45/100\n",
      "1185/1185 [==============================] - 0s 160us/step - loss: 0.7298 - acc: 0.7730 - val_loss: 0.7678 - val_acc: 0.8017\n",
      "Epoch 46/100\n",
      "1185/1185 [==============================] - 0s 164us/step - loss: 0.7207 - acc: 0.7873 - val_loss: 0.7517 - val_acc: 0.8137\n",
      "Epoch 47/100\n",
      "1185/1185 [==============================] - 0s 165us/step - loss: 0.7195 - acc: 0.7916 - val_loss: 0.7242 - val_acc: 0.8154\n",
      "Epoch 48/100\n",
      "1185/1185 [==============================] - 0s 139us/step - loss: 0.6791 - acc: 0.8076 - val_loss: 0.7233 - val_acc: 0.8137\n",
      "Epoch 49/100\n",
      "1185/1185 [==============================] - 0s 117us/step - loss: 0.6806 - acc: 0.8059 - val_loss: 0.7237 - val_acc: 0.8171\n",
      "Epoch 50/100\n",
      "1185/1185 [==============================] - 0s 143us/step - loss: 0.7070 - acc: 0.7806 - val_loss: 0.7319 - val_acc: 0.8103\n",
      "Epoch 51/100\n",
      "1185/1185 [==============================] - 0s 176us/step - loss: 0.6351 - acc: 0.8084 - val_loss: 0.7215 - val_acc: 0.8154\n",
      "Epoch 52/100\n",
      "1185/1185 [==============================] - 0s 172us/step - loss: 0.7280 - acc: 0.7966 - val_loss: 0.7447 - val_acc: 0.7983\n",
      "Epoch 53/100\n",
      "1185/1185 [==============================] - 0s 140us/step - loss: 0.6228 - acc: 0.8076 - val_loss: 0.7402 - val_acc: 0.7932\n",
      "Epoch 54/100\n",
      "1185/1185 [==============================] - 0s 119us/step - loss: 0.6558 - acc: 0.7857 - val_loss: 0.7251 - val_acc: 0.8051\n",
      "Epoch 55/100\n",
      "1185/1185 [==============================] - 0s 118us/step - loss: 0.6825 - acc: 0.7916 - val_loss: 0.7470 - val_acc: 0.8137\n",
      "Epoch 56/100\n",
      "1185/1185 [==============================] - 0s 116us/step - loss: 0.6326 - acc: 0.8068 - val_loss: 0.7466 - val_acc: 0.8103\n",
      "Epoch 57/100\n",
      "1185/1185 [==============================] - 0s 122us/step - loss: 0.6397 - acc: 0.7966 - val_loss: 0.7641 - val_acc: 0.8000\n",
      "Epoch 58/100\n",
      "1185/1185 [==============================] - 0s 118us/step - loss: 0.6724 - acc: 0.7958 - val_loss: 0.7387 - val_acc: 0.8017\n",
      "Epoch 59/100\n",
      "1185/1185 [==============================] - 0s 115us/step - loss: 0.6228 - acc: 0.8008 - val_loss: 0.7239 - val_acc: 0.8034\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1185/1185 [==============================] - 0s 128us/step - loss: 0.6050 - acc: 0.8186 - val_loss: 0.7262 - val_acc: 0.7983\n",
      "Epoch 61/100\n",
      "1185/1185 [==============================] - 0s 104us/step - loss: 0.6069 - acc: 0.8068 - val_loss: 0.7436 - val_acc: 0.8051\n",
      "Epoch 62/100\n",
      "1185/1185 [==============================] - 0s 106us/step - loss: 0.6323 - acc: 0.8093 - val_loss: 0.7665 - val_acc: 0.8000\n",
      "Epoch 63/100\n",
      "1185/1185 [==============================] - 0s 105us/step - loss: 0.5850 - acc: 0.8143 - val_loss: 0.7748 - val_acc: 0.7949\n",
      "Epoch 64/100\n",
      "1185/1185 [==============================] - 0s 105us/step - loss: 0.6492 - acc: 0.7966 - val_loss: 0.7609 - val_acc: 0.7983\n",
      "Epoch 65/100\n",
      "1185/1185 [==============================] - 0s 100us/step - loss: 0.5855 - acc: 0.8000 - val_loss: 0.7674 - val_acc: 0.8051\n",
      "Epoch 66/100\n",
      "1185/1185 [==============================] - 0s 107us/step - loss: 0.6452 - acc: 0.8017 - val_loss: 0.7774 - val_acc: 0.8017\n",
      "Epoch 67/100\n",
      "1185/1185 [==============================] - 0s 110us/step - loss: 0.6751 - acc: 0.8143 - val_loss: 0.7922 - val_acc: 0.8017\n",
      "Epoch 68/100\n",
      "1185/1185 [==============================] - 0s 113us/step - loss: 0.5957 - acc: 0.8101 - val_loss: 0.7698 - val_acc: 0.8154\n",
      "Epoch 69/100\n",
      "1185/1185 [==============================] - 0s 114us/step - loss: 0.6094 - acc: 0.8152 - val_loss: 0.7802 - val_acc: 0.8051\n",
      "Epoch 70/100\n",
      "1185/1185 [==============================] - 0s 110us/step - loss: 0.5866 - acc: 0.8346 - val_loss: 0.8016 - val_acc: 0.8000\n",
      "Epoch 71/100\n",
      "1185/1185 [==============================] - 0s 108us/step - loss: 0.5595 - acc: 0.8354 - val_loss: 0.7772 - val_acc: 0.8017\n",
      "Epoch 72/100\n",
      "1185/1185 [==============================] - 0s 116us/step - loss: 0.5635 - acc: 0.8414 - val_loss: 0.7596 - val_acc: 0.8154\n",
      "Epoch 73/100\n",
      "1185/1185 [==============================] - 0s 104us/step - loss: 0.5571 - acc: 0.8329 - val_loss: 0.7497 - val_acc: 0.8085\n",
      "Epoch 74/100\n",
      "1185/1185 [==============================] - 0s 105us/step - loss: 0.6059 - acc: 0.8093 - val_loss: 0.7487 - val_acc: 0.8068\n",
      "Epoch 75/100\n",
      "1185/1185 [==============================] - 0s 104us/step - loss: 0.6381 - acc: 0.8177 - val_loss: 0.7649 - val_acc: 0.8000\n",
      "Epoch 76/100\n",
      "1185/1185 [==============================] - 0s 108us/step - loss: 0.6389 - acc: 0.8110 - val_loss: 0.7548 - val_acc: 0.8205\n",
      "Epoch 77/100\n",
      "1185/1185 [==============================] - 0s 104us/step - loss: 0.6373 - acc: 0.8177 - val_loss: 0.7549 - val_acc: 0.8171\n",
      "Epoch 78/100\n",
      "1185/1185 [==============================] - 0s 108us/step - loss: 0.6002 - acc: 0.8228 - val_loss: 0.7658 - val_acc: 0.8034\n",
      "Epoch 79/100\n",
      "1185/1185 [==============================] - 0s 103us/step - loss: 0.5988 - acc: 0.8177 - val_loss: 0.7665 - val_acc: 0.8068\n",
      "Epoch 80/100\n",
      "1185/1185 [==============================] - 0s 111us/step - loss: 0.5681 - acc: 0.8143 - val_loss: 0.7769 - val_acc: 0.8068\n",
      "Epoch 81/100\n",
      "1185/1185 [==============================] - 0s 108us/step - loss: 0.5871 - acc: 0.8270 - val_loss: 0.8087 - val_acc: 0.8068\n",
      "Epoch 82/100\n",
      "1185/1185 [==============================] - 0s 108us/step - loss: 0.6015 - acc: 0.8042 - val_loss: 0.7695 - val_acc: 0.8000\n",
      "Epoch 83/100\n",
      "1185/1185 [==============================] - 0s 103us/step - loss: 0.5760 - acc: 0.8160 - val_loss: 0.7489 - val_acc: 0.8034\n",
      "Epoch 84/100\n",
      "1185/1185 [==============================] - 0s 112us/step - loss: 0.5946 - acc: 0.8127 - val_loss: 0.7740 - val_acc: 0.7966\n",
      "Epoch 85/100\n",
      "1185/1185 [==============================] - 0s 157us/step - loss: 0.5708 - acc: 0.8017 - val_loss: 0.8086 - val_acc: 0.7949\n",
      "Epoch 86/100\n",
      "1185/1185 [==============================] - 0s 176us/step - loss: 0.6085 - acc: 0.8068 - val_loss: 0.8158 - val_acc: 0.7761\n",
      "Epoch 87/100\n",
      "1185/1185 [==============================] - 0s 160us/step - loss: 0.7106 - acc: 0.7992 - val_loss: 0.8056 - val_acc: 0.7949\n",
      "Epoch 88/100\n",
      "1185/1185 [==============================] - 0s 131us/step - loss: 0.5666 - acc: 0.8278 - val_loss: 0.7714 - val_acc: 0.8051\n",
      "Epoch 89/100\n",
      "1185/1185 [==============================] - 0s 110us/step - loss: 0.6033 - acc: 0.8253 - val_loss: 0.7753 - val_acc: 0.7897\n",
      "Epoch 90/100\n",
      "1185/1185 [==============================] - 0s 121us/step - loss: 0.6380 - acc: 0.8135 - val_loss: 0.8262 - val_acc: 0.7795\n",
      "Epoch 91/100\n",
      "1185/1185 [==============================] - 0s 107us/step - loss: 0.6599 - acc: 0.7865 - val_loss: 0.7887 - val_acc: 0.8000\n",
      "Epoch 92/100\n",
      "1185/1185 [==============================] - 0s 123us/step - loss: 0.6526 - acc: 0.8186 - val_loss: 0.7568 - val_acc: 0.8085\n",
      "Epoch 93/100\n",
      "1185/1185 [==============================] - 0s 164us/step - loss: 0.6290 - acc: 0.8152 - val_loss: 0.7368 - val_acc: 0.8256\n",
      "Epoch 94/100\n",
      "1185/1185 [==============================] - 0s 169us/step - loss: 0.5958 - acc: 0.8186 - val_loss: 0.7350 - val_acc: 0.8256\n",
      "Epoch 95/100\n",
      "1185/1185 [==============================] - 0s 171us/step - loss: 0.5852 - acc: 0.8160 - val_loss: 0.7585 - val_acc: 0.8256\n",
      "Epoch 96/100\n",
      "1185/1185 [==============================] - 0s 174us/step - loss: 0.5574 - acc: 0.8295 - val_loss: 0.7745 - val_acc: 0.8085\n",
      "Epoch 97/100\n",
      "1185/1185 [==============================] - 0s 161us/step - loss: 0.5853 - acc: 0.8143 - val_loss: 0.7871 - val_acc: 0.8103\n",
      "Epoch 98/100\n",
      "1185/1185 [==============================] - 0s 163us/step - loss: 0.5078 - acc: 0.8278 - val_loss: 0.8048 - val_acc: 0.8103\n",
      "Epoch 99/100\n",
      "1185/1185 [==============================] - 0s 146us/step - loss: 0.5557 - acc: 0.8135 - val_loss: 0.7850 - val_acc: 0.8051\n",
      "Epoch 100/100\n",
      "1185/1185 [==============================] - 0s 121us/step - loss: 0.5945 - acc: 0.8186 - val_loss: 0.7757 - val_acc: 0.8120\n"
     ]
    }
   ],
   "source": [
    "cmlp_history = cmlp.fit(X_train,Y_train,\n",
    "                      batch_size=128,\n",
    "                      epochs=100,\n",
    "                      verbose=1,\n",
    "                      validation_split=.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build cnn\n",
    "scnn = Sequential()\n",
    "scnn.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "scnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "scnn.add(Dropout(0.5))\n",
    "scnn.add(Flatten())\n",
    "scnn.add(Dense(64, activation='relu'))\n",
    "scnn.add(Dropout(0.5))\n",
    "scnn.add(Dense(num_classes, activation='softmax'))\n",
    "scnn.compile(loss=\"categorical_crossentropy\",\n",
    "             optimizer='adadelta',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1185 samples, validate on 585 samples\n",
      "Epoch 1/5\n",
      "1185/1185 [==============================] - 2s 2ms/step - loss: 5.7325 - acc: 0.3384 - val_loss: 1.6596 - val_acc: 0.5675\n",
      "Epoch 2/5\n",
      "1185/1185 [==============================] - 2s 1ms/step - loss: 2.2212 - acc: 0.3662 - val_loss: 1.4606 - val_acc: 0.5504\n",
      "Epoch 3/5\n",
      "1185/1185 [==============================] - 2s 2ms/step - loss: 1.8646 - acc: 0.4641 - val_loss: 1.2116 - val_acc: 0.6308\n",
      "Epoch 4/5\n",
      "1185/1185 [==============================] - 2s 2ms/step - loss: 1.7179 - acc: 0.4574 - val_loss: 1.2557 - val_acc: 0.6308\n",
      "Epoch 5/5\n",
      "1185/1185 [==============================] - 2s 1ms/step - loss: 1.7130 - acc: 0.4574 - val_loss: 1.2037 - val_acc: 0.6308\n"
     ]
    }
   ],
   "source": [
    "scnn_history = scnn.fit(X_cnn_train,Y_train,\n",
    "                      batch_size=128,\n",
    "                      epochs=5,\n",
    "                      verbose=1,\n",
    "                      validation_split=.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Complex CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build cnn\n",
    "ccnn = Sequential()\n",
    "ccnn.add(Conv2D(16, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "ccnn.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "ccnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "ccnn.add(Dropout(0.33))\n",
    "ccnn.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "ccnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "ccnn.add(Dropout(0.33))\n",
    "ccnn.add(Flatten())\n",
    "ccnn.add(Dense(128, activation='relu'))\n",
    "ccnn.add(Dropout(0.5))\n",
    "ccnn.add(Dense(num_classes, activation='softmax'))\n",
    "ccnn.compile(loss=\"categorical_crossentropy\",\n",
    "             optimizer='adadelta',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1185 samples, validate on 585 samples\n",
      "Epoch 1/20\n",
      "1185/1185 [==============================] - 3s 2ms/step - loss: 0.9010 - acc: 0.6979 - val_loss: 0.8184 - val_acc: 0.7162\n",
      "Epoch 2/20\n",
      "1185/1185 [==============================] - 3s 3ms/step - loss: 0.8631 - acc: 0.6928 - val_loss: 0.7764 - val_acc: 0.7658\n",
      "Epoch 3/20\n",
      "1185/1185 [==============================] - 3s 2ms/step - loss: 0.8461 - acc: 0.7046 - val_loss: 0.9819 - val_acc: 0.7316\n",
      "Epoch 4/20\n",
      "1185/1185 [==============================] - 3s 3ms/step - loss: 0.7948 - acc: 0.7241 - val_loss: 0.7045 - val_acc: 0.7915\n",
      "Epoch 5/20\n",
      "1185/1185 [==============================] - 3s 2ms/step - loss: 0.7973 - acc: 0.7139 - val_loss: 0.7180 - val_acc: 0.7641\n",
      "Epoch 6/20\n",
      "1185/1185 [==============================] - 3s 2ms/step - loss: 0.7897 - acc: 0.7443 - val_loss: 0.7480 - val_acc: 0.7556\n",
      "Epoch 7/20\n",
      "1185/1185 [==============================] - 3s 3ms/step - loss: 0.7445 - acc: 0.7342 - val_loss: 0.6947 - val_acc: 0.7607\n",
      "Epoch 8/20\n",
      "1185/1185 [==============================] - 4s 3ms/step - loss: 0.7332 - acc: 0.7392 - val_loss: 0.6758 - val_acc: 0.7915\n",
      "Epoch 9/20\n",
      "1185/1185 [==============================] - 3s 2ms/step - loss: 0.7209 - acc: 0.7527 - val_loss: 0.6832 - val_acc: 0.7761\n",
      "Epoch 10/20\n",
      "1185/1185 [==============================] - 3s 2ms/step - loss: 0.6808 - acc: 0.7578 - val_loss: 0.6994 - val_acc: 0.7590\n",
      "Epoch 11/20\n",
      "1185/1185 [==============================] - 3s 2ms/step - loss: 0.7077 - acc: 0.7485 - val_loss: 0.6805 - val_acc: 0.7744\n",
      "Epoch 12/20\n",
      "1185/1185 [==============================] - 3s 2ms/step - loss: 0.7049 - acc: 0.7595 - val_loss: 0.6854 - val_acc: 0.7846\n",
      "Epoch 13/20\n",
      "1185/1185 [==============================] - 3s 3ms/step - loss: 0.6854 - acc: 0.7662 - val_loss: 0.6450 - val_acc: 0.7983\n",
      "Epoch 14/20\n",
      "1185/1185 [==============================] - 3s 3ms/step - loss: 0.6561 - acc: 0.7570 - val_loss: 0.6957 - val_acc: 0.7726\n",
      "Epoch 15/20\n",
      "1185/1185 [==============================] - 3s 3ms/step - loss: 0.6664 - acc: 0.7705 - val_loss: 0.6578 - val_acc: 0.7778\n",
      "Epoch 16/20\n",
      "1185/1185 [==============================] - 3s 3ms/step - loss: 0.6364 - acc: 0.7637 - val_loss: 0.6703 - val_acc: 0.7795\n",
      "Epoch 17/20\n",
      "1185/1185 [==============================] - 3s 3ms/step - loss: 0.6515 - acc: 0.7696 - val_loss: 0.6498 - val_acc: 0.8000\n",
      "Epoch 18/20\n",
      "1185/1185 [==============================] - 3s 3ms/step - loss: 0.6089 - acc: 0.8025 - val_loss: 0.6516 - val_acc: 0.7932\n",
      "Epoch 19/20\n",
      "1185/1185 [==============================] - 3s 3ms/step - loss: 0.6192 - acc: 0.7873 - val_loss: 0.6402 - val_acc: 0.7915\n",
      "Epoch 20/20\n",
      "1185/1185 [==============================] - 3s 3ms/step - loss: 0.5936 - acc: 0.7840 - val_loss: 0.6374 - val_acc: 0.8085\n"
     ]
    }
   ],
   "source": [
    "ccnn_history = ccnn.fit(X_cnn_train,Y_train,\n",
    "                        batch_size=16,\n",
    "                        epochs=20,\n",
    "                        verbose=1,\n",
    "                        validation_split=.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. LSTM RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_hidden = 32\n",
    "col_hidden = 32\n",
    "row, col, pixel = X_cnn.shape[1:]\n",
    "x = Input(shape=(row, col, pixel))\n",
    "\n",
    "encoded_rows = TimeDistributed(LSTM(row_hidden))(x)\n",
    "encoded_columns = LSTM(col_hidden)(encoded_rows)\n",
    "\n",
    "prediction = Dense(num_classes, activation='softmax')(encoded_columns)\n",
    "rnn = Model(x, prediction)\n",
    "rnn.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1185 samples, validate on 585 samples\n",
      "Epoch 1/5\n",
      "1185/1185 [==============================] - 19s 16ms/step - loss: 1.8404 - acc: 0.3283 - val_loss: 1.6459 - val_acc: 0.4359\n",
      "Epoch 2/5\n",
      "1185/1185 [==============================] - 18s 15ms/step - loss: 1.6635 - acc: 0.3992 - val_loss: 1.5924 - val_acc: 0.4359\n",
      "Epoch 3/5\n",
      "1185/1185 [==============================] - 18s 15ms/step - loss: 1.6026 - acc: 0.4118 - val_loss: 1.5011 - val_acc: 0.4650\n",
      "Epoch 4/5\n",
      "1185/1185 [==============================] - 18s 15ms/step - loss: 1.5025 - acc: 0.4329 - val_loss: 1.4457 - val_acc: 0.4752\n",
      "Epoch 5/5\n",
      "1185/1185 [==============================] - 19s 16ms/step - loss: 1.5033 - acc: 0.4262 - val_loss: 1.4773 - val_acc: 0.4667\n"
     ]
    }
   ],
   "source": [
    "rnn_history = rnn.fit(X_cnn_train,Y_train,\n",
    "                      batch_size=8,\n",
    "                      epochs=5,\n",
    "                      verbose=1,\n",
    "                      validation_split=.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Simple MLP': 0.75,\n",
       " 'Complex MLP': 0.7809633027522935,\n",
       " 'Simple CNN': 0.6055045871559633,\n",
       " 'Complex CNN': 0.7706422018348624,\n",
       " 'LSTM RNN': 0.463302752293578}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = {\"Simple MLP\":0.0,\n",
    "          \"Complex MLP\":0.0,\n",
    "          \"Simple CNN\":0.0,\n",
    "          \"Complex CNN\":0.0,\n",
    "          \"LSTM RNN\":0.0\n",
    "         }\n",
    "_,scores[\"Simple MLP\"] = mlp.evaluate(X_test,Y_test,verbose=0)\n",
    "_,scores[\"Complex MLP\"] = cmlp.evaluate(X_test,Y_test,verbose=0)\n",
    "_,scores[\"Simple CNN\"] = scnn.evaluate(X_cnn_test,Y_test,verbose=0)\n",
    "_,scores[\"Complex CNN\"] = ccnn.evaluate(X_cnn_test,Y_test,verbose=0)\n",
    "_,scores[\"LSTM RNN\"] = rnn.evaluate(X_cnn_test,Y_test,verbose=0)\n",
    "display(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging by the holdout group, the Complex MLP performed the best achieving an accuracy of 78.09%. The other complex model came in second, scoring within 1%. My assumption that the CNNs would perform best did not hold true here. A simple SVM classifier was able to perform similarly well. Interestingly, the RNN did not perform well. These models are known for performing well with sequential data, and although music is sequential, we did not feed the model sequential features. If I were to deploy a model I may consider the best performer as training time did not vary considerably between these methods. For production, I would choose the Complex MLP for its high score and moderate runtime.  \n",
    "<br>Further research may include gathering more data and generating different features. While there was a sizeable amount of data present, neural networks are known to perform better on large datasets. The MFCC features are widely regarded as vital features in audio recognition, however, that does not exclude the efficiency of other audio features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
